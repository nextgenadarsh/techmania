{"ast":null,"code":"const QnAs = [{\n  q: \"What is CAP Theorem?\",\n  a: `**The CAP Theorem for distributed computing** was published by Eric Brewer. This states that it is not possible for a distributed computer system to simultaneously provide all three of the following guarantees:\n\n- Consistency (all nodes see the same data even at the same time with concurrent updates)\n- Availability (a guarantee that every request receives a response about whether it was successful or failed)\n- Partition tolerance (the system continues to operate despite arbitrary message loss or failure of part of the system)\n\nThe CAP acronym corresponds to these three guarantees. This theorem has created the base for modern distributed computing approaches. Worlds most high volume traffic companies (e.g. Amazon, Google, Facebook) use this as basis for deciding their application architecture. It's important to understand that only two of these three conditions can be guaranteed to be met by a system.`,\n  tags: ['architecture']\n}, {\n  q: \"Explain the difference between Asynchronous and Parallel programming?\",\n  a: `When you run something asynchronously it means it is non-blocking, you execute it without waiting for it to complete and carry on with other things. Parallelism means to run multiple things at the same time, in parallel. Parallelism works well when you can separate tasks into independent pieces of work. Async and Callbacks are generally a way (tool or mechanism) to express concurrency i.e. a set of entities possibly talking to each other and sharing resources.\n\nTake for example rendering frames of a 3D animation. To render the animation takes a long time so if you were to launch that render from within your animation editing software you would make sure it was running asynchronously so it didn't lock up your UI and you could continue doing other things. Now, each frame of that animation can also be considered as an individual task. If we have multiple CPUs/Cores or multiple machines available, we can render multiple frames in parallel to speed up the overall workload.\n    `\n}, {\n  q: \"What Is Scalability?\",\n  a: `\nScalability is the ability of a system, network, or process to handle a growing amount of load by adding more resources. The adding of resource can be done in two ways\n\n- **Scaling Up**\n  - This involves adding more resources to the existing nodes. For example, adding more RAM, Storage or processing power.\n\n- Scaling Out\n  - This involves adding more nodes to support more users.\n\nAny of the approaches can be used for scaling up/out a application, however the cost of adding resources (per user) may change as the volume increases. If we add resources to the system It should increase the ability of application to take more load in a proportional manner of added resources.\n\nAn ideal application should be able to serve high level of load in less resources. However, in practical, linearly scalable system may be the best option achievable. Poorly designed applications may have really high cost on scaling up/out since it will require more resources/user as the load increases.`\n}, {\n  q: \"Explain the difference between Asynchronous and Parallel programming?\",\n  a: `When you run something asynchronously it means it is non-blocking, you execute it without waiting for it to complete and carry on with other things. Parallelism means to run multiple things at the same time, in parallel. Parallelism works well when you can separate tasks into independent pieces of work. Async and Callbacks are generally a way (tool or mechanism) to express concurrency i.e. a set of entities possibly talking to each other and sharing resources.\n\n    Take for example rendering frames of a 3D animation. To render the animation takes a long time so if you were to launch that render from within your animation editing software you would make sure it was running asynchronously so it didn't lock up your UI and you could continue doing other things. Now, each frame of that animation can also be considered as an individual task. If we have multiple CPUs/Cores or multiple machines available, we can render multiple frames in parallel to speed up the overall workload.`\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}, {\n  q: \"\",\n  a: ``\n}];\nexport default QnAs;","map":{"version":3,"sources":["/Users/adarsh/Documents/github/techmania/src/routes/qna-game/data/index.js"],"names":["QnAs","q","a","tags"],"mappings":"AAAA,MAAMA,IAAI,GAAG,CACX;AACEC,EAAAA,CAAC,EAAE,sBADL;AAEEC,EAAAA,CAAC,EAAG;AACR;AACA;AACA;AACA;AACA;AACA,6XARE;AASEC,EAAAA,IAAI,EAAE,CAAC,cAAD;AATR,CADW,EAYX;AACEF,EAAAA,CAAC,EAAE,uEADL;AAEEC,EAAAA,CAAC,EAAG;AACR;AACA;AACA;AALE,CAZW,EAmBX;AACED,EAAAA,CAAC,EAAE,sBADL;AAEEC,EAAAA,CAAC,EAAG;AACR;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAbE,CAnBW,EAkCX;AACED,EAAAA,CAAC,EAAE,uEADL;AAEEC,EAAAA,CAAC,EAAG;AACR;AACA;AAJE,CAlCW,EAwCX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CAxCW,EA4CX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CA5CW,EAgDX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CAhDW,EAoDX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CApDW,EAwDX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CAxDW,EA4DX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CA5DW,EAgEX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CAhEW,EAoEX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CApEW,EAwEX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CAxEW,EA4EX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CA5EW,EAgFX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CAhFW,EAoFX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CApFW,EAwFX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CAxFW,EA4FX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CA5FW,EAgGX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CAhGW,EAoGX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CApGW,EAwGX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CAxGW,EA4GX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CA5GW,EAgHX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CAhHW,EAoHX;AACED,EAAAA,CAAC,EAAE,EADL;AAEEC,EAAAA,CAAC,EAAG;AAFN,CApHW,CAAb;AA0HA,eAAeF,IAAf","sourcesContent":["const QnAs = [\n  {\n    q: \"What is CAP Theorem?\",\n    a: `**The CAP Theorem for distributed computing** was published by Eric Brewer. This states that it is not possible for a distributed computer system to simultaneously provide all three of the following guarantees:\n\n- Consistency (all nodes see the same data even at the same time with concurrent updates)\n- Availability (a guarantee that every request receives a response about whether it was successful or failed)\n- Partition tolerance (the system continues to operate despite arbitrary message loss or failure of part of the system)\n\nThe CAP acronym corresponds to these three guarantees. This theorem has created the base for modern distributed computing approaches. Worlds most high volume traffic companies (e.g. Amazon, Google, Facebook) use this as basis for deciding their application architecture. It's important to understand that only two of these three conditions can be guaranteed to be met by a system.`,\n    tags: ['architecture']\n  },\n  {\n    q: \"Explain the difference between Asynchronous and Parallel programming?\",\n    a: `When you run something asynchronously it means it is non-blocking, you execute it without waiting for it to complete and carry on with other things. Parallelism means to run multiple things at the same time, in parallel. Parallelism works well when you can separate tasks into independent pieces of work. Async and Callbacks are generally a way (tool or mechanism) to express concurrency i.e. a set of entities possibly talking to each other and sharing resources.\n\nTake for example rendering frames of a 3D animation. To render the animation takes a long time so if you were to launch that render from within your animation editing software you would make sure it was running asynchronously so it didn't lock up your UI and you could continue doing other things. Now, each frame of that animation can also be considered as an individual task. If we have multiple CPUs/Cores or multiple machines available, we can render multiple frames in parallel to speed up the overall workload.\n    `\n  },\n  {\n    q: \"What Is Scalability?\",\n    a: `\nScalability is the ability of a system, network, or process to handle a growing amount of load by adding more resources. The adding of resource can be done in two ways\n\n- **Scaling Up**\n  - This involves adding more resources to the existing nodes. For example, adding more RAM, Storage or processing power.\n\n- Scaling Out\n  - This involves adding more nodes to support more users.\n\nAny of the approaches can be used for scaling up/out a application, however the cost of adding resources (per user) may change as the volume increases. If we add resources to the system It should increase the ability of application to take more load in a proportional manner of added resources.\n\nAn ideal application should be able to serve high level of load in less resources. However, in practical, linearly scalable system may be the best option achievable. Poorly designed applications may have really high cost on scaling up/out since it will require more resources/user as the load increases.`\n  },\n  {\n    q: \"Explain the difference between Asynchronous and Parallel programming?\",\n    a: `When you run something asynchronously it means it is non-blocking, you execute it without waiting for it to complete and carry on with other things. Parallelism means to run multiple things at the same time, in parallel. Parallelism works well when you can separate tasks into independent pieces of work. Async and Callbacks are generally a way (tool or mechanism) to express concurrency i.e. a set of entities possibly talking to each other and sharing resources.\n\n    Take for example rendering frames of a 3D animation. To render the animation takes a long time so if you were to launch that render from within your animation editing software you would make sure it was running asynchronously so it didn't lock up your UI and you could continue doing other things. Now, each frame of that animation can also be considered as an individual task. If we have multiple CPUs/Cores or multiple machines available, we can render multiple frames in parallel to speed up the overall workload.`\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n  {\n    q: \"\",\n    a: ``\n  },\n];\n\nexport default QnAs;"]},"metadata":{},"sourceType":"module"}